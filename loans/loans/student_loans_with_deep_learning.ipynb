{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7ZNhGcW6q6r"
   },
   "source": [
    "# Student Loan Risk with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/tensorflow/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ed/1a/b4ab4b8f8b3a41fade4899fd00b5b2d2dad0981f3e1bb10df4c522975fd7/tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/ed/1a/b4ab4b8f8b3a41fade4899fd00b5b2d2dad0981f3e1bb10df4c522975fd7/tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/ed/1a/b4ab4b8f8b3a41fade4899fd00b5b2d2dad0981f3e1bb10df4c522975fd7/tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/ed/1a/b4ab4b8f8b3a41fade4899fd00b5b2d2dad0981f3e1bb10df4c522975fd7/tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/15/db/7f731524fe0e56c6b2eb57d05b55d3badd80ef7d1f1ed59db191b2fdd8ab/protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/44/66/10773d9ea847ba0ae5c36478333d92c6dae3396205bf18091910f63f3ee9/tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/7e/61/79618621c56244d4778811f98737af8be4af1c0b034261c270cdfbda0762/grpcio-1.62.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.62.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.16,>=2.15 from https://files.pythonhosted.org/packages/37/12/f6e9b9dcc310263cbd3948274e286538bd6800fd0c268850788f14a0c6d0/tensorboard-2.15.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.16,>=2.15.0 from https://files.pythonhosted.org/packages/b6/c8/2f823c8958d5342eafc6dd3e922f0cc4fcf8c2e0460284cc462dae3b60a0/tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras<2.16,>=2.15.0 from https://files.pythonhosted.org/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/b7/1d/f152a5f6d243b6acbb2a710ed19aa47154d678359bed995abdd9daf0cff0/google_auth-2.28.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for google-auth-oauthlib<2,>=0.5 from https://files.pythonhosted.org/packages/71/bf/9e125754d1adb3bc4bd206c4e5df756513b1d23675ac06caa471278d1f3f/google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/6f/bb/5deac77a9af870143c684ab46a7934038a53eb4aa975bc0687ed6ca2c610/requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/antitheft/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached grpcio-1.62.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0otrXpJc6q6u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 18:17:40.861744: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-27 18:17:40.905975: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 18:17:40.906012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 18:17:40.907242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 18:17:40.914272: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-27 18:17:40.934808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 18:17:43.058747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpV4Y-3Z6q6w"
   },
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUuSzp2l6q6w"
   },
   "source": [
    "### Step 1: Read the `student-loans.csv` file into a Pandas DataFrame. Review the DataFrame, looking for columns that could eventually define your features and target variables.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "G65km1KD6q6x",
    "outputId": "93d12d8d-c415-4017-8452-5b4966e4dde5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_history</th>\n",
       "      <th>location_parameter</th>\n",
       "      <th>stem_degree_score</th>\n",
       "      <th>gpa_ranking</th>\n",
       "      <th>alumni_success</th>\n",
       "      <th>study_major_code</th>\n",
       "      <th>time_to_completion</th>\n",
       "      <th>finance_workshop_score</th>\n",
       "      <th>cohort_ranking</th>\n",
       "      <th>total_loan_score</th>\n",
       "      <th>financial_aid_score</th>\n",
       "      <th>credit_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_history  location_parameter  stem_degree_score  gpa_ranking  \\\n",
       "0              7.4                0.70               0.00          1.9   \n",
       "1              7.8                0.88               0.00          2.6   \n",
       "2              7.8                0.76               0.04          2.3   \n",
       "3             11.2                0.28               0.56          1.9   \n",
       "4              7.4                0.70               0.00          1.9   \n",
       "\n",
       "   alumni_success  study_major_code  time_to_completion  \\\n",
       "0           0.076              11.0                34.0   \n",
       "1           0.098              25.0                67.0   \n",
       "2           0.092              15.0                54.0   \n",
       "3           0.075              17.0                60.0   \n",
       "4           0.076              11.0                34.0   \n",
       "\n",
       "   finance_workshop_score  cohort_ranking  total_loan_score  \\\n",
       "0                  0.9978            3.51              0.56   \n",
       "1                  0.9968            3.20              0.68   \n",
       "2                  0.9970            3.26              0.65   \n",
       "3                  0.9980            3.16              0.58   \n",
       "4                  0.9978            3.51              0.56   \n",
       "\n",
       "   financial_aid_score  credit_ranking  \n",
       "0                  9.4               0  \n",
       "1                  9.8               0  \n",
       "2                  9.8               0  \n",
       "3                  9.8               1  \n",
       "4                  9.4               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv into a Pandas DataFrame\n",
    "file_path = \"https://static.bc-edx.com/ai/ail-v-1-0/m18/lms/datasets/student-loans.csv\"\n",
    "loans_df = pd.read_csv(file_path)\n",
    "\n",
    "# Review the DataFrame\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8ZYB8wx6q6x",
    "outputId": "b509fc34-4488-406e-e451-2069fec37371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payment_history           float64\n",
       "location_parameter        float64\n",
       "stem_degree_score         float64\n",
       "gpa_ranking               float64\n",
       "alumni_success            float64\n",
       "study_major_code          float64\n",
       "time_to_completion        float64\n",
       "finance_workshop_score    float64\n",
       "cohort_ranking            float64\n",
       "total_loan_score          float64\n",
       "financial_aid_score       float64\n",
       "credit_ranking              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the data types associated with the columns\n",
    "loans_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P8aX-dW75JO",
    "outputId": "63251fa8-5ac1-4112-c2f7-bc5d97ea8491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_ranking\n",
       "1    855\n",
       "0    744\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the credit_ranking value counts\n",
    "loans_df[\"credit_ranking\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6vbZeDH6q6y"
   },
   "source": [
    "### Step 2: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “credit_ranking”. The remaining columns should define the features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5eVAP5M6q6y",
    "outputId": "89728bbf-6930-4573-a126-9f1b66ed8859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of target set (y):\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: credit_ranking, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the target set y using the credit_ranking column\n",
    "\n",
    "\n",
    "# Display a sample of y\n",
    "# Assuming you have already imported pandas and performed the necessary data preprocessing steps\n",
    "\n",
    "# Define the target set y using the credit_ranking column\n",
    "y = loans_df[\"credit_ranking\"]\n",
    "\n",
    "# Display a sample of y\n",
    "print(\"Sample of target set (y):\")\n",
    "print(y.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "IIkrD2Sn6q6z",
    "outputId": "ce07c4a3-fb81-4657-d11e-ec85ae8554c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features set (X):\n",
      "   payment_history  location_parameter  stem_degree_score  gpa_ranking  \\\n",
      "0              7.4                0.70               0.00          1.9   \n",
      "1              7.8                0.88               0.00          2.6   \n",
      "2              7.8                0.76               0.04          2.3   \n",
      "3             11.2                0.28               0.56          1.9   \n",
      "4              7.4                0.70               0.00          1.9   \n",
      "\n",
      "   alumni_success  study_major_code  time_to_completion  \\\n",
      "0           0.076              11.0                34.0   \n",
      "1           0.098              25.0                67.0   \n",
      "2           0.092              15.0                54.0   \n",
      "3           0.075              17.0                60.0   \n",
      "4           0.076              11.0                34.0   \n",
      "\n",
      "   finance_workshop_score  cohort_ranking  total_loan_score  \\\n",
      "0                  0.9978            3.51              0.56   \n",
      "1                  0.9968            3.20              0.68   \n",
      "2                  0.9970            3.26              0.65   \n",
      "3                  0.9980            3.16              0.58   \n",
      "4                  0.9978            3.51              0.56   \n",
      "\n",
      "   financial_aid_score  \n",
      "0                  9.4  \n",
      "1                  9.8  \n",
      "2                  9.8  \n",
      "3                  9.8  \n",
      "4                  9.4  \n"
     ]
    }
   ],
   "source": [
    "# Define features set X by selecting all columns but credit_ranking\n",
    "\n",
    "\n",
    "# Review the features DataFrame\n",
    "\n",
    "# Assuming you have already imported pandas and performed the necessary data preprocessing steps\n",
    "\n",
    "# Define features set X by selecting all columns but credit_ranking\n",
    "X = loans_df.drop(\"credit_ranking\", axis=1)\n",
    "\n",
    "# Review the features DataFrame\n",
    "print(\"Features set (X):\")\n",
    "print(X.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmM9c-tj6q6z"
   },
   "source": [
    "### Step 3: Split the features and target sets into training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OD7xwU_96q6z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1279, 11)\n",
      "Shape of X_test: (320, 11)\n",
      "Shape of y_train: (1279,)\n",
      "Shape of y_test: (320,)\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X and y have been defined in the previous steps\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Display the shapes of the resulting datasets for verification\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9i6DHY06q6z"
   },
   "source": [
    "### Step 4: Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BzD3z20m6q6z"
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train and X_test have been defined in the previous steps\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the features testing dataset using the scaler fitted to the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZzVDjba6q6z"
   },
   "source": [
    "---\n",
    "\n",
    "## Compile and Evaluate a Model Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-pSux4Q6q60"
   },
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5C94FCd6q60",
    "outputId": "cbf05783-2f56-4745-cd33-649a6152e510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11\n"
     ]
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "\n",
    "\n",
    "# Review the number of features\n",
    "\n",
    "# Assuming X_train_scaled has been defined in the previous steps\n",
    "\n",
    "# Define the number of inputs (features) to the model\n",
    "num_inputs = X_train_scaled.shape[1]\n",
    "\n",
    "# Display the number of features\n",
    "print(\"Number of features:\", num_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c_KXDLkF6q60"
   },
   "outputs": [],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 = 64\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 = 32\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "output_nodes = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "63UdFncw6q60"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Assuming num_inputs, hidden_nodes_layer1, hidden_nodes_layer2, and output_nodes have been defined\n",
    "\n",
    "# Create the Sequential model instance\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with the specified number of nodes and ReLU activation function\n",
    "model.add(Dense(units=hidden_nodes_layer1, activation='relu', input_dim=num_inputs))\n",
    "\n",
    "# Add the second hidden layer with the specified number of nodes and ReLU activation function\n",
    "model.add(Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Add the output layer with the specified number of output neurons (for binary classification) and sigmoid activation function\n",
    "model.add(Dense(units=output_nodes, activation='sigmoid'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Beoh4f_6q61",
    "outputId": "2e50f810-086b-4d89-bf7d-98afbe0d649d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                768       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2881 (11.25 KB)\n",
      "Trainable params: 2881 (11.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the Sequential model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRqWGIRo6q61"
   },
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E-hZaeSn6q61"
   },
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "# Compile the Sequential model with binary_crossentropy loss function, adam optimizer, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x25e8Idc6q61",
    "outputId": "e95946ba-23da-47a3-a1c1-5e9a2a484a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6560\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7467\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7506\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7553\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7553\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7631\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7639\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7639\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7654\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7694\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7701\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7748\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7701\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.7803\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.7858\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.7889\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7873\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7842\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.7912\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8006\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.7959\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.7967\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8069\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8053\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.8030\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.8124\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4072 - accuracy: 0.8100\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8155\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8194\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8210\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3947 - accuracy: 0.8264\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8272\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3874 - accuracy: 0.8280\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8327\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8303\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8319\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8342\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8428\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.8350\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8366\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8444\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8428\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8468\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.8444\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8491\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3469 - accuracy: 0.8499\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8491\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8483\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8577\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8585\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "# Assuming X_train_scaled and y_train have been defined\n",
    "\n",
    "# Fit the Sequential model to the training data for 50 epochs\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfHMPZVI6q61"
   },
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hfVADKo6q61",
    "outputId": "7df473ad-3301-4b49-e5c3-16e1687cc1cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7344\n",
      "\n",
      "Evaluation Results:\n",
      "Test Loss: 0.5560\n",
      "Test Accuracy: 73.44%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "# Assuming X_test_scaled and y_test have been defined\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "evaluation_results = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Test Loss: {evaluation_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {evaluation_results[1]*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpAv0rXA6q61"
   },
   "source": [
    "### Step 4: Save and export your model to a keras file, and name the file `student_loans.keras`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "q0MetN0W6q61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved to /home/antitheft/Documents/loans/student_loans.keras\n"
     ]
    }
   ],
   "source": [
    "# Set the model's file path\n",
    "\n",
    "\n",
    "# Export your model to a keras file\n",
    "\n",
    "# Set the model's file path\n",
    "model_file_path = \"/home/antitheft/Documents/loans/student_loans.keras\"\n",
    "\n",
    "# Save the model to the specified file path\n",
    "model.save(model_file_path)\n",
    "\n",
    "# Display a message indicating that the model has been saved\n",
    "print(f\"Model has been saved to {model_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1opCDdN6q61"
   },
   "source": [
    "---\n",
    "## Predict Loan Repayment Success by Using your Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfIfpeiy6q61"
   },
   "source": [
    "### Step 1: Reload your saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OCET2mvW6q61"
   },
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "\n",
    "\n",
    "# Load the model to a new object\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Set the model's file path\n",
    "model_file_path = \"/home/antitheft/Documents/loans/student_loans.keras\"\n",
    "\n",
    "# Load the model to a new object\n",
    "loaded_model = load_model(model_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTPKooGw6q61"
   },
   "source": [
    "### Step 2: Make predictions on the testing data and save the predictions to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vet7qjgx6q62",
    "outputId": "0925af42-7e12-4978-8396-2a2c1580e1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "Sample Predictions: [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the test data\n",
    "\n",
    "\n",
    "# Display a sample of the predictions\n",
    "\n",
    "# Assuming X_test_scaled has been defined\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Round the predictions to binary values (0 or 1)\n",
    "rounded_predictions = [round(pred[0]) for pred in predictions]\n",
    "\n",
    "# Display a sample of the predictions\n",
    "sample_predictions = rounded_predictions[:10]  # Displaying the first 10 predictions as an example\n",
    "print(\"Sample Predictions:\", sample_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "87o8exFPhjfl",
    "outputId": "da5339c5-cea7-43e4-ec22-e168ea16dfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions DataFrame:\n",
      "      Actual  Predicted\n",
      "75         0          0\n",
      "1283       1          0\n",
      "408        1          1\n",
      "1281       1          1\n",
      "1118       1          1\n"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a DataFrame and round the predictions to binary results\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming y_test has been defined\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': rounded_predictions\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Predictions DataFrame:\")\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxxLwycg6q62"
   },
   "source": [
    "### Step 4: Display a classification report with the y test data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTxYZibW6q67",
    "outputId": "f341b396-9b4c-478c-dba8-f6d904ba10e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71       154\n",
      "           1       0.73      0.78      0.75       166\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.74      0.73      0.73       320\n",
      "weighted avg       0.73      0.73      0.73       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report with the y test data and predictions\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report with the y test data and predictions\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, rounded_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Aaof1tBtcp6"
   },
   "source": [
    "---\n",
    "## Discuss creating a recommendation system for student loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CC8cNpNtcp6"
   },
   "source": [
    "Briefly answer the following questions in the space provided:\n",
    "\n",
    "1. Describe the data that you would need to collect to build a recommendation system to recommend student loan options for students. Explain why this data would be relevant and appropriate.\n",
    "\n",
    "2. Based on the data you chose to use in this recommendation system, would your model be using collaborative filtering, content-based filtering, or context-based filtering? Justify why the data you selected would be suitable for your choice of filtering method.\n",
    "\n",
    "3. Describe two real-world challenges that you would take into consideration while building a recommendation system for student loans. Explain why these challenges would be of concern for a student loan recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KqIT8kYtcp6"
   },
   "source": [
    "**1. Describe the data that you would need to collect to build a recommendation system to recommend student loan options for students. Explain why this data would be relevant and appropriate.**\n",
    "\n",
    "\n",
    "**2. Based on the data you chose to use in this recommendation system, would your model be using collaborative filtering, content-based filtering, or context-based filtering? Justify why the data you selected would be suitable for your choice of filtering method.**\n",
    "\n",
    "\n",
    "**3. Describe two real-world challenges that you would take into consideration while building a recommendation system for student loans. Explain why these challenges would be of concern for a student loan recommendation system.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the data that you would need to collect to build a recommendation system to recommend student loan options for students. Explain why this data would be relevant and appropriate.\n",
    "\n",
    "To build a recommendation system for student loan options, relevant data would include:\n",
    "\n",
    "Student Information: Personal details such as age, location, academic history, and field of study.\n",
    "Financial Information: Income, credit history, and existing financial commitments.\n",
    "Loan History: Any existing student loans, repayment history, and credit ranking.\n",
    "Loan Options Data: Features and terms of available loan options, interest rates, and repayment plans.\n",
    "Career Aspirations: Future career plans, expected income post-graduation.\n",
    "This data is crucial for assessing a student's financial situation, creditworthiness, and preferences. By understanding their academic and career goals, the recommendation system can suggest loan options that align with their needs and financial capabilities.\n",
    "\n",
    "2. Based on the data you chose to use in this recommendation system, would your model be using collaborative filtering, content-based filtering, or context-based filtering? Justify why the data you selected would be suitable for your choice of filtering method.\n",
    "\n",
    "For a student loan recommendation system, a combination of collaborative and content-based filtering would be appropriate:\n",
    "\n",
    "Collaborative Filtering: It considers the behavior and preferences of similar students to recommend loans. It can be effective when students with similar academic and financial backgrounds tend to make similar choices.\n",
    "\n",
    "Content-Based Filtering: It leverages information about the students and loan options to make recommendations. This is valuable when considering a student's individual financial situation, academic history, and career aspirations.\n",
    "\n",
    "The combination allows for a more personalized recommendation by considering both user behavior and the characteristics of loan options.\n",
    "\n",
    "3. Describe two real-world challenges that you would take into consideration while building a recommendation system for student loans. Explain why these challenges would be of concern for a student loan recommendation system.\n",
    "\n",
    "Data Privacy and Security: Collecting sensitive information about students' financial situations raises concerns about data privacy and security. Ensuring robust security measures and obtaining informed consent from users is crucial to build trust and comply with regulations.\n",
    "\n",
    "Dynamic Financial Situations: Financial situations of students can change rapidly, especially during their academic journey. Adapting to these changes, providing real-time updates, and offering flexible recommendations that align with the evolving financial context are challenges to address. Regular updates and feedback mechanisms would be necessary to keep recommendations relevant."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
